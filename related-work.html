<h1 id="fluid-media-format">Fluid Media Format</h1>
<h2 id="research-thoughts">Research thoughts</h2>
<ul>
<li>My intuition is that a main research contribution of our work is to systematically study the forms of reference and alignment between different forms of existing multimodal media, produce knowledge about how authors are doing that or want to do that, and propose a high-level language-based way of specifying that in a DDF. </li>
<li>How do we SYSTEMATICALLY study/code the forms of reference, alignment, juxtaposition between different forms of media?</li>
<li>How can we scope this DOWN! Maybe just &quot;aligned text&quot;?</li>
<li>What are existing ways of composing aligned &amp; referenced media?
<ul>
<li>&quot;timed text&quot;—the YT editor is pretty slick</li>
<li>karaoke annotation</li>
<li>LRC (subtitle file)</li>
<li>aligned translations</li>
<li>omg, WHAT??? https://en.wikipedia.org/wiki/Synchronized_Multimedia_Integration_Language</li>
<li>1999???</li>
<li>https://en.wikipedia.org/wiki/Timed_text</li>
<li>https://en.wikipedia.org/wiki/Timed_Text_Markup_Language</li>
<li>AUTHORING TOOLS? https://en.wikipedia.org/wiki/Synchronized_Multimedia_Integration_Language#Authoring_tools</li>
<li>WAIT IT WORKS WITH SVG THOUGH!!</li>
<li>https://en.wikipedia.org/wiki/HTML%2BTIME</li>
<li>i truly am a multimedia person...bring it back</li>
<li>Why are we seeing a resurgence in dynamic media? Or why are we not? Why did SMIL die? What is the difference between multimedia and whatever today is—dynamic media? Hypermedia? Why am I so obsessed with scrollytelling?</li>
<li>I Guess.... Deep Learning + MOOCs + rise of internet + mainstream &quot;hypermedia&quot; (now it's just media)? 2002 vs 2020. Now we can synthesize speech, now we can automatically caption media and tag visuals in video, things are richer. I mean, it's NYT scrollytelling, doesn't get more mainstream than now</li>
<li>Like, why isn't Amy Pavel's stuff hypermedia</li>
</ul></li>
<li></li>
<li>Max: Something I want to grapple with personally for this project is the relative importance of encoding existing material into empirically determined primitives vs Reigniting student agency by deconstructing the content that was &quot;flattened&quot; in the first place. The representation in the lecturer's brain will always be richer than the student's, and the best tools reify these representations to play with live</li>
<li>just thinking out loud some more: I want to steer the tool away from reinforcing the status quo on the student end. Students already are given lectures, videos, slides, and sparse explorables, if the DDF is just a compiler then all it helps are teachers (analogy: end users don't care about clang vs gcc).If the mappings aren't erased after compilation, and students benefit from the richer DDF representation, then we have a medium that's better than existing ones.</li>
<li>3b1b videos are awesome, literate tutorials are awesome, and maybe the state of education would be better if we had '''more''' of them, but I do want to explore the richer axis we were brainstorming where the content is less flat</li>
<li>richer media I'm not 100% sure what is &quot;better&quot; or &quot;more effective&quot; but I do like the aligned content with a rich underlying data structure like the ones we've been collecting</li>
<li>but really any video, slides, etc which is more hypertext-like: where there's an underlying set of data that renders it and lets you query and interrogate the data semantically through the host medium of video/slides/etc</li>
<li>like clicking &quot;x&quot; at 5:34 of some khan video and seeing where it appears and maybe a history of its manipulations</li>
<li>and further (which doesn't really exist anywhere), a bricolage &quot;script&quot; environment where you can see the flattened content in context with all the broader ideas they were pulled from (?)</li>
<li>Yeah, I like all these ideas, and I do think the idea of compilation makes these new media possible. I think in a paper, it would be great to demo 1-2 of these formats to show the claim that the richer/structure-preserving representation enables new capabilities without much additional work.</li>
<li>We can repro all these existing diagrams... with these magic features that are enabled by a good design8:37Note that the magic part is more than just a cherry on top of the Penrose paper, it's part of why it's so fundamentally compelling/exciting, and we go to great lengths to show what the system can do that Illustrator can't</li>
</ul>
<h2 id="research-questions">Research questions</h2>
<p>The key research challenges are:</p>
<ul>
<li>Is it actually possible to design an IR that compiles to many different media formats?</li>
<li>How do we design an IR that’s both readable/authorable to end-users and supports common machine analyses on creating dynamic media?</li>
<li>How do we fit existing handmade dynamic artifacts into this framework?</li>
</ul>
<p>Other questions include:</p>
<ul>
<li>What are the design goals?</li>
<li>What are the other research challenges?</li>
<li>What are the output media formats?</li>
<li>What existing examples can we to reproduce?</li>
<li>How to augment existing examples?
<ul>
<li>Existing examples of augmenting existing examples?</li>
</ul></li>
<li>Example breakdowns of existing examples into language?</li>
<li>Example recreations/augmentations/remixes of existing examples via the compiler?</li>
<li>Communities and people to interview?</li>
<li>How do existing languages / approaches work?</li>
</ul>
<h2 id="related-work">Related work</h2>
<ul>
<li>Questionnaire:
<ul>
<li>Sources</li>
<li>Name</li>
<li>Creators</li>
<li>Dates active</li>
<li>Purpose</li>
<li>Language features</li>
<li>Input</li>
<li>Output</li>
<li>Limitations</li>
</ul></li>
</ul>
<h3 id="ginga-ncl-date">Ginga / NCL (date?)</h3>
<ul>
<li>Sources</li>
<li>Ginga-NCL http://www.iginga.org/files/biblio/2010_06_soares.pdf</li>
<li>NCL Page tutorials
<ul>
<li>http://www.ncl.org.br/en/tutorials</li>
<li>http://club.ncl.org.br/node/31</li>
<li>WebNCL https://dl.acm.org/doi/10.1145/2382636.2382719</li>
<li>NCL Composer http://composer.telemidia.puc-rio.br/doku.php/en/start?redirect=1</li>
</ul></li>
</ul>
<h3 id="smil-xhtml-smil-html-time-date">SMIL, XHTML + SMIL, HTML + TIME (date?)</h3>
<p><img src="media/SMIL.png" width=500></p>
<ul>
<li>Sources
<ul>
<li>SMIL authoring systems</li>
<li>TTML https://en.wikipedia.org/wiki/Timed_Text_Markup_Language</li>
<li>SMIL + Time https://homepages.cwi.nl/~jack/presentations/smilstate-for-rwab.pdf</li>
<li>Retro AF SMIL tutorial https://homepages.cwi.nl/~media/SMIL/Tutorial/SMILTut.html</li>
<li>timesheets.js http://wam.inrialpes.fr/timesheets/</li>
<li>https://ics.utc.fr/c2m/res/TimesheetsSoftware.pdf</li>
<li>SMIL is dead</li>
</ul></li>
<li>Notes
<ul>
<li>Synchronized Media Integration Language</li>
<li>XML-based language for describing multimedia presentations</li>
<li>Seems to be dead since mid-2000s</li>
</ul></li>
</ul>
<h3 id="videopuppet-httpswww.videopuppet.com">VideoPuppet https://www.videopuppet.com/</h3>
<p>TODO</p>
<h4 id="idyll">Idyll</h4>
<p>TODO</p>
<h4 id="manim">manim</h4>
<p>TODO</p>
<h4 id="misc.">Misc.</h4>
<ul>
<li>YouTube Timed Text editor https://www.youtube.com/timedtext_editor?action_mde_edit_form=1&amp;v=Kas0tIxDvrg&amp;lang=en&amp;bl=vmp&amp;ui=hd&amp;ref=player&amp;tab=captions&amp;ar=1585953093029&amp;o=U</li>
<li>&quot;Video Digests&quot; (Pavel, Agrawala, et al.) http://vis.berkeley.edu/papers/videodigests/videodigests_small.pdf</li>
</ul>
<h2 id="examples">Examples</h2>
<p>General examples of multimodal alignment:</p>
<ul>
<li>TEXT/IMAGE/SOUND: math explainer videos / slides</li>
<li>TEXT/SOUND: music videos / karaoke videos / podcasts</li>
<li>TEXT/IMAGE: reactive articles (e.g. incremental visualization)</li>
<li>TEXT/TEXT: subtitles / translation</li>
<li>TEXT/INTERACTION: What football will look like in the future, &quot;What is code&quot;...</li>
<li>https://en.wikipedia.org/wiki/17776</li>
<li>https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/</li>
<li>The series was developed by Graham MacAree, who used a Vox Media tool that creates custom packages from standard article sets to give Bois creative leeway and to accommodate the series' weight on the SB Nation website. MacAree found that there were few resources online for achieving the desired effects.[4]</li>
<li>As of Monday, &quot;17776&quot; had massive reader engagement. Fay Sliger, communications director at Vox Media, said in an email that the project has had more than 2.3 million pageviews since its publication and an average engagement time of more than 9 minutes. About 43 percent of readers — of which more than half are viewing the piece on mobile devices — had finished each installment of the series as of Friday.</li>
</ul>
<p>Input formats: TODO</p>
<p>Output formats:</p>
<ul>
<li>Video with voiceover</li>
<li>Print book</li>
<li>Podcast</li>
<li>Scrolling/interactive webpage</li>
<li>(Web) Slides with builds and speaker notes</li>
</ul>
<p>Conclusions: Synchronization, alignment, and reference matter! And are hard tasks that can benefit from automation, and free up iteration time. Quotes:</p>
<blockquote>
<p>In 2016, two friends and I released an opensource tool and wanted to make some nice demos and tutorial videos. When creating videos, I ended up spending most of the time doing things that have nothing to do with the content, but with the boring tasks around synchronisation and alignment.</p>
</blockquote>
<p>(VideoPuppet)</p>
<blockquote>
<p>&quot;I find myself converting my written text into slides and then into video, but I'm always pointing at stuff (like a subpart of a diagram) in my slides and can't reference it in speech, so then I have to make these very granular builds for the slides, but then I have trouble syncing up what I'm saying to the builds&quot;</p>
</blockquote>
<p>(Faculty member in CS)</p>
<p>Examples of artifacts:</p>
<ul>
<li>Jonathan Corum example http://style.org/tapestry/</li>
<li>Ways of Hearing: podcast to book https://mitpress.mit.edu/books/ways-hearing</li>
</ul>
